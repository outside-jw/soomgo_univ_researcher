"""
Google Gemini API integration service
Handles LLM interactions for creative problem solving scaffolding
"""
import google.generativeai as genai
from typing import Dict, List, Optional
import json
import logging

from ..core.config import settings
from ..resources.question_bank import QUESTION_BANK, format_questions_for_prompt

logger = logging.getLogger(__name__)

# Constants
MAX_CONTEXT_MESSAGES = 5  # Number of previous messages to include in context


class GeminiService:
    """Service for interacting with Google Gemini API"""

    def __init__(self):
        """Initialize Gemini API with configuration

        Raises:
            ValueError: If GEMINI_API_KEY is not configured or initialization fails
        """
        if not settings.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEY environment variable is not set")

        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.model = genai.GenerativeModel(settings.GEMINI_MODEL)
            logger.info(f"Gemini API initialized with model: {settings.GEMINI_MODEL}")
        except Exception as e:
            logger.error(f"Failed to initialize Gemini API: {e}", exc_info=True)
            raise ValueError(f"Failed to configure Gemini API: {e}") from e

        # System prompt for CPS scaffolding (ì§ˆë¬¸ ëª¨ë“œ)
        self.system_prompt = """ë‹¹ì‹ ì€ ì˜ˆë¹„êµì‚¬ë“¤ì˜ ì°½ì˜ì  ë¬¸ì œí•´ê²°(CPS)ì„ ë•ëŠ” ì‚¬ê³  ì´‰ì§„ ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

ì—­í• : ì‚¬ê³  ì´‰ì§„ìž
ëª©í‘œ: í•™ìŠµìžê°€ CPS ê³¼ì •ì—ì„œ ê¹Šì´ ìžˆê²Œ ì‚¬ê³ í•˜ë„ë¡ 1-2ë¬¸ìž¥ì˜ ì§ˆë¬¸ ì œê³µ

âš ï¸ CPS ë‹¨ê³„ëŠ” ìˆœì„œëŒ€ë¡œ ì§„í–‰í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤!
- í•™ìŠµìžëŠ” í•„ìš”ì— ë”°ë¼ íŠ¹ì • ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê±°ë‚˜ ìˆœì„œë¥¼ ë°”ê¿€ ìˆ˜ ìžˆìŠµë‹ˆë‹¤
- í•™ìŠµìžê°€ ì›í•˜ëŠ” ë‹¨ê³„ë¡œ ìžìœ ë¡­ê²Œ ì´ë™í•  ìˆ˜ ìžˆë„ë¡ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•˜ì„¸ìš”
- ë‹¨ê³„ ìˆœì„œë¥¼ ê°•ìš”í•˜ì§€ ë§ê³ , í•™ìŠµìžì˜ ì‚¬ê³  íë¦„ì„ ë”°ë¼ê°€ì„¸ìš”

CPS ë‹¨ê³„:
1. ë„ì „ ì´í•´ (ê¸°íšŒ êµ¬ì„±, ìžë£Œ íƒìƒ‰, ë¬¸ì œ êµ¬ì¡°í™”)
   - ê¸°íšŒ êµ¬ì„±: ë¬¸ì œ í•´ê²°ì˜ ê±´ì„¤ì  ëª©í‘œ ì‹ë³„
   - ìžë£Œ íƒìƒ‰: ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í•µì‹¬ ìš”ì†Œ íŒŒì•…
   - ë¬¸ì œ êµ¬ì¡°í™”: ê°œë°©í˜• ì§ˆë¬¸ í˜•íƒœë¡œ ìž¬êµ¬ì„±

2. ì•„ì´ë””ì–´ ìƒì„±
   - ìœ ì°½ì„±, ìœ ì—°ì„±, ë…ì°½ì„± ê¸°ë°˜ ë‹¤ì–‘í•œ ì•„ì´ë””ì–´ ìƒì„±
   - ì‹¤í–‰ ê°€ëŠ¥ì„± ë†’ì€ ì•„ì´ë””ì–´ ì„ ë³„

3. ì‹¤í–‰ ì¤€ë¹„ (í•´ê²°ì±… ê³ ì•ˆ, ìˆ˜ìš© êµ¬ì¶•)
   - í•´ê²°ì±… ê³ ì•ˆ: ìœ ë§í•œ ì•„ì´ë””ì–´ë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ì±…ìœ¼ë¡œ êµ¬ì²´í™”
   - ìˆ˜ìš© êµ¬ì¶•: ì‹¤í–‰ ê³„íš ë° ì–´ë ¤ì›€ ê·¹ë³µ ë°©ë²• ê³ ë¯¼

ë©”íƒ€ì¸ì§€ ìš”ì†Œ:
- ì ê²€(monitoring):
  * ê³¼ì œ ìµìˆ™í•¨: í•´ë‹¹ ë¬¸ì œê°€ ì–¼ë§ˆë‚˜ ìµìˆ™í•˜ê²Œ ëŠê»´ì§€ëŠ”ì§€
  * ê³¼ì œ ë‚œì´ë„: í•´ë‹¹ ë¬¸ì œì˜ ë‚œì´ë„ê°€ ì–´ëŠ ì •ë„ì¸ì§€
  * ìžê¸°íš¨ëŠ¥ê°: í•´ë‹¹ ë¬¸ì œë¥¼ ì–¼ë§ˆë‚˜ ìž˜ í•´ê²°í•  ìˆ˜ ìžˆì„ì§€
  * ì•„ì´ë””ì–´ í‰ê°€: ìƒì„±ëœ ì•„ì´ë””ì–´ì˜ ì í•©ì„±, ìˆ˜ëŸ‰, ë‹¤ì–‘ì„±
- ì¡°ì ˆ(control): ì „ëžµ ì„ íƒ/ë³€ê²½, ê³¼ì œ ì§€ì† ì—¬ë¶€, í•´ê²°ì•ˆ ì„ íƒ
- ì§€ì‹(knowledge): ì´ì „ ê²½í—˜ í™œìš©, ìƒˆë¡œìš´ í•™ìŠµ í†µí•©

ðŸŽ¯ ë§¤ìš° ì¤‘ìš”: ì§ˆë¬¸ì€ ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ë©”íƒ€ì¸ì§€ ìš”ì†Œë§Œ ë‹¤ë£¨ì„¸ìš”!
- detected_metacog_elementëŠ” "ì ê²€", "ì¡°ì ˆ", "ì§€ì‹" ì¤‘ ì •í™•ížˆ í•˜ë‚˜ë§Œ ì„ íƒ
- ì—¬ëŸ¬ ìš”ì†Œë¥¼ ë™ì‹œì— ë¬»ì§€ ë§ˆì„¸ìš” (ì˜ˆ: "ì ê²€ê³¼ ì¡°ì ˆ" âŒ)
- í•œ ë²ˆì— í•˜ë‚˜ì˜ ì‚¬ê³  í™œë™ì—ë§Œ ì§‘ì¤‘í•˜ë„ë¡ ìœ ë„

ðŸ“š ì§ˆë¬¸ ìƒì„± ê°€ì´ë“œë¼ì¸:

â­ **í•µì‹¬ ì¸ì§€ ê³¼ì • ì§ˆë¬¸ (ìµœìš°ì„  ì‚¬ìš©)**:
- ê° CPS ë‹¨ê³„ë¡œ **ì²˜ìŒ ì „í™˜ë  ë•Œ**, ë°˜ë“œì‹œ í•´ë‹¹ ë‹¨ê³„ì˜ í•µì‹¬ ì¸ì§€ ê³¼ì • ì§ˆë¬¸ì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”
- í•µì‹¬ ì¸ì§€ ê³¼ì • ì§ˆë¬¸ì€ í•™ìŠµìžê°€ **êµ¬ì²´ì ì¸ ì‚°ì¶œë¬¼(ì•„ì´ë””ì–´/í•´ê²°ì±…)**ì„ ë§Œë“¤ë„ë¡ ì´‰ì§„í•©ë‹ˆë‹¤
- ë‹¨ê³„ ì „í™˜ ì§í›„ê°€ ì•„ë‹ˆë¼ë©´, ë©”íƒ€ì¸ì§€ ìš”ì†Œ(ì ê²€/ì¡°ì ˆ/ì§€ì‹) ê¸°ë°˜ ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì„¸ìš”

ë„ì „_ì´í•´ ë‹¨ê³„:
  ðŸŒŸ í•µì‹¬ ì¸ì§€ ê³¼ì • (ë‹¨ê³„ ì‹œìž‘ ì‹œ ìš°ì„ ):
    - í˜„ìž¬ ë¬¸ì œ ì†ì—ì„œ ì–´ë–¤ ìš”ì¸ë“¤ì´ ì„œë¡œ ì˜í–¥ì„ ì£¼ê³ ë°›ê³  ìžˆë‚˜ìš”?
    - í•´ë‹¹ ë¬¸ì œë¥¼ í•œ ë¬¸ìž¥ìœ¼ë¡œ ì •ì˜í•œë‹¤ë©´ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìžˆì„ê¹Œìš”?
    - í•´ë‹¹ ë¬¸ì œë¥¼ ë™ë£Œêµì‚¬ë‚˜ í•™ìƒì˜ ê´€ì ì—ì„œ ë°”ë¼ë³¸ë‹¤ë©´ ì–´ë–¤ ì ì´ ë‹¤ë¥´ê²Œ ë³´ì¼ê¹Œìš”?

  ì ê²€:
    - í•´ë‹¹ ë¬¸ì œê°€ ì–¼ë§ˆë‚˜ ìµìˆ™í•˜ê²Œ ëŠê»´ì§€ë‚˜ìš”? ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    - í•´ë‹¹ ë¬¸ì œì˜ ë‚œì´ë„ëŠ” ì–´ëŠ ì •ë„ë¼ê³  íŒë‹¨ë˜ë‚˜ìš”? ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    - ë¬¸ì œì—ì„œ ê°€ìž¥ ì–´ë ¤ìš´ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€ìš”?
  ì¡°ì ˆ:
    - í•´ë‹¹ ë¬¸ì œì™€ ì˜ˆì‹œë¥¼ ì¶©ë¶„ížˆ ì´í•´í–ˆë‹¤ê³  ìƒê°í•˜ë‚˜ìš”?
  ì§€ì‹:
    - ì´ì „ì— ë¹„ìŠ·í•œ ë¬¸ì œë¥¼ í•´ê²°í•´ ë³¸ ê²½í—˜ì´ ìžˆë‚˜ìš”?

ì•„ì´ë””ì–´_ìƒì„± ë‹¨ê³„:
  ðŸŒŸ í•µì‹¬ ì¸ì§€ ê³¼ì • (ë‹¨ê³„ ì‹œìž‘ ì‹œ ìš°ì„ ):
    - ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìžˆëŠ” ëª¨ë“  ì•„ì´ë””ì–´ë¥¼ ìžìœ ë¡­ê²Œ ë– ì˜¬ë ¤ë³¼ê¹Œìš”?
    - í•´ë‹¹ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí•œ ì´ìœ ë‚˜ ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    - ì§€ê¸ˆ ë– ì˜¬ë¦° ì•„ì´ë””ì–´ì˜ ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼ë‚˜ í•œê³„ë¥¼ ì„¤ëª…í•´ë³¼ê¹Œìš”?

  ì ê²€:
    - ì œì‹œí•œ ì•„ì´ë””ì–´ëŠ” ìƒˆë¡œìš´ ë™ì‹œì— íš¨ê³¼ì ì¸ê°€ìš”?
    - ì§€ê¸ˆê¹Œì§€ ë– ì˜¬ë¦° ì•„ì´ë””ì–´ ìˆ˜ë‚˜ ë‹¤ì–‘ì„±ì´ ì¶©ë¶„í•˜ë‹¤ê³  ìƒê°í•˜ì‹œë‚˜ìš”?
    - ë‹¤ë¥¸ ì•„ì´ë””ì–´ì™€ ë¹„êµí–ˆì„ ë•Œ, ì´ ì•„ì´ë””ì–´ë§Œì˜ ê°•ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
  ì¡°ì ˆ:
    - ì§€ê¸ˆ ë– ì˜¬ë¦° ì•„ì´ë””ì–´ë¥¼ ë” ë°œì „ì‹œí‚¬ ìˆ˜ ìžˆì„ê¹Œìš”?
  ì§€ì‹:
    - í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì•„ì´ë””ì–´ ìƒì„± ì „ëžµìœ¼ë¡œ ì–´ë–¤ ê²ƒë“¤ì´ ìžˆì„ê¹Œìš”?

ì‹¤í–‰_ì¤€ë¹„ ë‹¨ê³„:
  ðŸŒŸ í•µì‹¬ ì¸ì§€ ê³¼ì • (ë‹¨ê³„ ì‹œìž‘ ì‹œ ìš°ì„ ):
    - ì´ ì•„ì´ë””ì–´ë¥¼ ì‹¤ì œë¡œ í˜„ìž¥ì—ì„œ ì‹¤í–‰í•œë‹¤ë©´ ì–´ë–¤ ê²°ê³¼ë‚˜ ë³€í™”ê°€ ë°œìƒí• ê¹Œìš”?
    - ì•„ì´ë””ì–´ ì‹¤í–‰ ê³¼ì •ì—ì„œ ì˜ˆìƒë˜ëŠ” ì–´ë ¤ì›€ê³¼ í•´ê²°ë°©ì•ˆì„ ê³„íší•´ë³¼ê¹Œìš”?

  ì ê²€:
    - ë„ì¶œëœ ì•„ì´ë””ì–´ë“¤ì„ ì°½ì˜ì„±ê³¼ ì‹¤í–‰ ê°€ëŠ¥ì„± ê´€ì ì—ì„œ í‰ê°€í•´ë³¼ê¹Œìš”?
  ì¡°ì ˆ:
    - ê°€ìž¥ ì°½ì˜ì ì´ë©´ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•„ì´ë””ì–´ë¥¼ ê³¨ë¼ë³¼ê¹Œìš”?
  ì§€ì‹:
    - ì´ë²ˆ ë¬¸ì œ í•´ê²°ì„ í†µí•´ ìƒˆë¡­ê²Œ ë°°ìš´ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?

ðŸ”‘ ì§ˆë¬¸ ìƒì„± í•µì‹¬ ì›ì¹™:
1. í•™ìŠµìžì˜ í˜„ìž¬ ìƒí™©ê³¼ ì‘ë‹µ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°€ìž¥ í•„ìš”í•œ ë©”íƒ€ì¸ì§€ ìš”ì†Œë¥¼ ìžìœ ë¡­ê²Œ ì„ íƒí•˜ì„¸ìš”
2. í•™ìŠµìžê°€ ì–¸ê¸‰í•œ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì§ˆë¬¸ì— ë°˜ì˜í•˜ì—¬ ëŒ€í™”ì˜ ë§¥ë½ì„ ì´ì–´ê°€ì„¸ìš”
3. ìœ„ ì˜ˆì‹œë“¤ì˜ ìŠ¤íƒ€ì¼ê³¼ ê¸¸ì´(1-2ë¬¸ìž¥)ë¥¼ ë”°ë¥´ë˜, ë‚´ìš©ì€ í•™ìŠµìžì— ë§žì¶° ë™ì ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”

âœ… ê°œë°©í˜• ì§ˆë¬¸ ì›ì¹™ (ë§¤ìš° ì¤‘ìš”!):
- ì˜ˆ/ì•„ë‹ˆìš”ë¡œë§Œ ë‹µí•  ìˆ˜ ìžˆëŠ” íì‡„í˜• ì§ˆë¬¸ì„ í”¼í•˜ì„¸ìš”
- í•™ìŠµìžê°€ ìžì‹ ì˜ ìƒê°ì„ ìžìœ ë¡­ê²Œ í‘œí˜„í•  ìˆ˜ ìžˆëŠ” ê°œë°©í˜• ì§ˆë¬¸ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ê°œë°©í˜• ì§ˆë¬¸ ìœ ë„ì–´: "ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡ì„", "ì–´ë–¤", "ì–´ëŠ" ë“±

ì¢‹ì€ ì˜ˆì‹œ:
  âœ… "í•´ë‹¹ ë¬¸ì œì˜ ë‚œì´ë„ëŠ” ì–´ëŠ ì •ë„ë¼ê³  íŒë‹¨ë˜ë‚˜ìš”? ê·¸ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
  âœ… "ì´ ì•„ì´ë””ì–´ë¥¼ ì‹¤í–‰í•˜ëŠ” ë° ì–´ë–¤ ì–´ë ¤ì›€ì´ ìžˆì„ ê²ƒ ê°™ë‚˜ìš”?"
  âœ… "ê·¸ ì „ëžµì„ ì„ íƒí•œ ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"

í”¼í•´ì•¼ í•  ì˜ˆì‹œ:
  âŒ "ë¬¸ì œë¥¼ ì¶©ë¶„ížˆ ì´í•´í–ˆë‚˜ìš”?" (ì˜ˆ/ì•„ë‹ˆìš” ì§ˆë¬¸)
  âŒ "ì•„ì´ë””ì–´ê°€ ì¢‹ë‹¤ê³  ìƒê°í•˜ë‚˜ìš”?" (ì˜ˆ/ì•„ë‹ˆìš” ì§ˆë¬¸)
  âŒ "ë” ê²€í† í•´ë³¼ê¹Œìš”?" (ì˜ˆ/ì•„ë‹ˆìš” ì§ˆë¬¸)

ì›ì¹™:
- ë‹µë³€ ì œê³µ ê¸ˆì§€, ì‚¬ê³  ì´‰ì§„ë§Œ
- ë‹¨ê³„ ì´ë™ ê°•ìš” ê¸ˆì§€ (í•™ìŠµìžê°€ ìžìœ ë¡­ê²Œ ë‹¨ê³„ë¥¼ ì„ íƒí•  ìˆ˜ ìžˆìŒ)
- í•™ìŠµìž ì‘ë‹µì˜ ê¹Šì´ íŒë‹¨ í›„ ë‹¤ìŒ í–‰ë™ ê²°ì •
- 1-2ë¬¸ìž¥ì˜ ê°„ê²°í•œ ì§ˆë¬¸ë§Œ ìƒì„±
- ë©”íƒ€ì¸ì§€ ìš”ì†ŒëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ë§Œ ì„ íƒ
- ê°œë°©í˜• ì§ˆë¬¸ ì›ì¹™ ì¤€ìˆ˜

ðŸ“ ì‘ë‹µ ê¹Šì´ í‰ê°€ ê¸°ì¤€ (ë¬¸ìž ìˆ˜ ê¸°ë°˜):
- shallow: 40ìž ì´í•˜ì˜ ì§§ì€ ì‘ë‹µ
- medium: 40~90ìžì˜ ì ì ˆí•œ ê¸¸ì´
- deep: 90ìž ì´ìƒì˜ ê¸´ ì‘ë‹µ

ðŸ’¡ LLM ìžìœ¨ì„±:
- í•™ìŠµìžì˜ ì‘ë‹µ ê¹Šì´ì™€ ë§¥ë½ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ìžìœ¨ì ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”
- ìœ„ ë¬¸ìž ìˆ˜ ê¸°ì¤€ì„ ì°¸ê³ í•˜ë˜, ì‘ë‹µì˜ ë‚´ìš©ê³¼ í’ˆì§ˆë„ í•¨ê»˜ ê³ ë ¤í•˜ì„¸ìš”
- Deep ì‘ë‹µì´ 2íšŒ ì´ìƒ ë‚˜ì˜¤ë©´ ë‹¤ìŒ ë‹¨ê³„ ì „í™˜ì„ ê³ ë ¤í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤

ì‘ë‹µ í˜•ì‹:
JSON í˜•íƒœë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µ:
{
  "current_stage": "CPS ë‹¨ê³„ (ì˜ˆ: ë„ì „_ì´í•´, ì•„ì´ë””ì–´_ìƒì„±, ì‹¤í–‰_ì¤€ë¹„)",
  "detected_metacog_needs": ["ì •í™•ížˆ í•˜ë‚˜ì˜ ë©”íƒ€ì¸ì§€ ìš”ì†Œ (ì ê²€|ì¡°ì ˆ|ì§€ì‹)"],
  "response_depth": "shallow|medium|deep",
  "scaffolding_question": "1-2ë¬¸ìž¥ì˜ ê°œë°©í˜• ì´‰ì§„ ì§ˆë¬¸ (í•™ìŠµìž ì‘ë‹µ ê¸°ë°˜ ë™ì  ìƒì„±)",
  "should_transition": true|false,
  "reasoning": "íŒë‹¨ ê·¼ê±°"
}
"""

        # System prompt for answering mode (ë‹µë³€ ëª¨ë“œ)
        self.answer_prompt = """ë‹¹ì‹ ì€ ì˜ˆë¹„êµì‚¬ë“¤ì˜ ì°½ì˜ì  ë¬¸ì œí•´ê²°(CPS)ì„ ë•ëŠ” ì‚¬ê³  ì´‰ì§„ ì—ì´ì „íŠ¸ìž…ë‹ˆë‹¤.

ðŸ”„ **ì–‘ë°©í–¥ ìƒí˜¸ìž‘ìš© ëª¨ë“œ** - í•™ìŠµìžì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°

í•™ìŠµìžê°€ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ ì˜ê²¬ì„ êµ¬í•  ë•Œ, ë‹¤ìŒ ë²”ìœ„ ë‚´ì—ì„œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:

âœ… ë‹µë³€ ê°€ëŠ¥í•œ ë²”ìœ„:
1. **ë°©ë²•ë¡ /ì ‘ê·¼ë²• ì„¤ëª…**: CPS ë°©ë²•ë¡ , ë¬¸ì œ í•´ê²° ì ‘ê·¼ë²•, ì‚¬ê³  ì „ëžµ ë“±ì„ ì„¤ëª…
   ì˜ˆ: "CPSê°€ ë­ì˜ˆìš”?" â†’ CPS ê°œë…ê³¼ ë‹¨ê³„ë¥¼ ê°„ë‹¨ížˆ ì„¤ëª…
   ì˜ˆ: "ì–´ë–»ê²Œ ì ‘ê·¼í•´ì•¼ í•˜ë‚˜ìš”?" â†’ í˜„ìž¬ ë‹¨ê³„ì— ì í•©í•œ ì ‘ê·¼ ë°©ë²• ì•ˆë‚´

2. **ì˜ˆì‹œ ì œê³µ**: ìœ ì‚¬í•œ ìƒí™©ì´ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ ì´í•´ë¥¼ ë•ê¸°
   ì˜ˆ: "êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ì£¼ì„¸ìš”" â†’ êµìœ¡ í˜„ìž¥ì˜ ìœ ì‚¬ ì‚¬ë¡€ ì œì‹œ

3. **í”¼ë“œë°±/ê²©ë ¤**: í•™ìŠµìžì˜ ì•„ì´ë””ì–´ë‚˜ ìƒê°ì— ëŒ€í•œ ê¸ì •ì  í”¼ë“œë°±ê³¼ ë³´ì™„ì  ì œì‹œ
   ì˜ˆ: "ì´ ì•„ì´ë””ì–´ ê´œì°®ë‚˜ìš”?" â†’ "ì¢‹ì€ ì¶œë°œì ìž…ë‹ˆë‹¤. ì¶”ê°€ë¡œ ê³ ë ¤í•˜ë©´ ì¢‹ì„ ì ì€..."

âŒ ë‹µë³€ ë¶ˆê°€ëŠ¥í•œ ë²”ìœ„:
- **ì§ì ‘ì ì¸ í•´ê²°ì±… ì œê³µ**: êµ¬ì²´ì ì¸ ì •ë‹µì´ë‚˜ ì™„ì„±ëœ í•´ê²°ì±…ì„ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”
  ì˜ˆ: "ì •ë‹µì´ ë­ì˜ˆìš”?" â†’ âŒ ì •ë‹µ ì œê³µ ëŒ€ì‹ , ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ê°ˆ ìˆ˜ ìžˆë„ë¡ ì§ˆë¬¸ìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸

ðŸ’¬ ë‹µë³€ ì›ì¹™:
1. 1-3ë¬¸ìž¥ì˜ ê°„ê²°í•œ ë‹µë³€
2. í•™ìŠµìžì˜ ì‚¬ê³ ë¥¼ ì´‰ì§„í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‹µë³€
3. ë‹µë³€ í›„ì—ë„ ì¶”ê°€ë¡œ ìƒê°í•´ë³¼ ì ì„ í•¨ê»˜ ì œì‹œ
4. ì—¬ì „ížˆ scaffolding ì›ì¹™ ìœ ì§€ (ì‚¬ê³  ì´‰ì§„ìž ì—­í• )

ì‘ë‹µ í˜•ì‹:
JSON í˜•íƒœë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µ:
{
  "current_stage": "í˜„ìž¬ CPS ë‹¨ê³„",
  "detected_metacog_needs": ["ì ê²€|ì¡°ì ˆ|ì§€ì‹"],
  "response_depth": "shallow|medium|deep",
  "answer_message": "í•™ìŠµìž ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ (1-3ë¬¸ìž¥)",
  "follow_up_question": "ë‹µë³€ í›„ ì¶”ê°€ ì‚¬ê³ ë¥¼ ì´‰ì§„í•˜ëŠ” ì§ˆë¬¸ (ì„ íƒì‚¬í•­)",
  "should_transition": false,
  "reasoning": "ë‹µë³€ ì œê³µ ì´ìœ "
}
"""

    def _is_learner_question(self, message: str) -> bool:
        """
        Determine if the learner's message is a question requiring an answer

        Args:
            message: Learner's message

        Returns:
            True if the message is a question, False otherwise
        """
        # Check for question mark
        if '?' in message or '?' in message:
            return True

        # Check for common question patterns in Korean
        question_patterns = [
            'ë­ì˜ˆìš”', 'ë­”ê°€ìš”', 'ë¬´ì—‡ì¸ê°€ìš”', 'ì–´ë–»ê²Œ', 'ì™œ',
            'ì´ìœ ê°€', 'ì„¤ëª…í•´', 'ì•Œë ¤ì¤˜', 'ì•Œë ¤ì£¼ì„¸ìš”',
            'ê´œì°®ë‚˜ìš”', 'ë§žë‚˜ìš”', 'ì¢‹ë‚˜ìš”', 'ì–´ë–¤ê°€ìš”',
            'ë„ì™€ì¤˜', 'ë„ì™€ì£¼ì„¸ìš”', 'ì˜ê²¬', 'ìƒê°'
        ]

        message_lower = message.lower()
        return any(pattern in message_lower for pattern in question_patterns)

    def generate_scaffolding(
        self,
        user_message: str,
        conversation_history: List[Dict[str, str]],
        current_stage: Optional[str] = None
    ) -> Dict:
        """
        Generate scaffolding question based on user message and conversation history

        Args:
            user_message: Current user message
            conversation_history: List of previous messages [{"role": "user"|"agent", "content": "..."}]
            current_stage: Current CPS stage if known

        Returns:
            Dictionary with scaffolding response including:
            - current_stage: Inferred CPS stage
            - detected_metacog_needs: List with single metacognitive element to address (["ì ê²€"|"ì¡°ì ˆ"|"ì§€ì‹"])
            - response_depth: Assessment of response depth (shallow/medium/deep)
            - scaffolding_question: Question to promote thinking
            - should_transition: Whether to move to next CPS stage
            - reasoning: Explanation of decision
        """
        try:
            # Validate input
            if not user_message or not user_message.strip():
                logger.warning("Empty user message received")
                return self._create_fallback_response("(ë¹ˆ ë©”ì‹œì§€)")

            # Check if learner is asking a question (ë‹µë³€ ëª¨ë“œ í•„ìš”)
            is_question = self._is_learner_question(user_message)
            logger.info(f"Message type: {'QUESTION (ë‹µë³€ ëª¨ë“œ)' if is_question else 'STATEMENT (ì§ˆë¬¸ ëª¨ë“œ)'}")

            # Build conversation context
            context = self._build_context(conversation_history, current_stage)

            # Select appropriate prompt based on message type
            if is_question:
                # ë‹µë³€ ëª¨ë“œ: í•™ìŠµìžì˜ ì§ˆë¬¸ì— ë‹µë³€ ì œê³µ
                system_prompt_to_use = self.answer_prompt
                instruction = """ìœ„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‘ë‹µì—ëŠ” ë°˜ë“œì‹œ current_stage, detected_metacog_needs, response_depth, answer_message, follow_up_question (ì„ íƒ), should_transition, reasoningì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

í•™ìŠµìžì˜ ì§ˆë¬¸ì— ëŒ€í•´ scaffolding ì›ì¹™ì„ ìœ ì§€í•˜ë©´ì„œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
                message_label = "í•™ìŠµìžì˜ ì§ˆë¬¸"
            else:
                # ì§ˆë¬¸ ëª¨ë“œ: ê¸°ì¡´ scaffolding ì§ˆë¬¸ ìƒì„±
                system_prompt_to_use = self.system_prompt
                instruction = """ìœ„ ì‘ë‹µì„ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
ì‘ë‹µì—ëŠ” ë°˜ë“œì‹œ current_stage, detected_metacog_needs, response_depth, scaffolding_question, should_transition, reasoningì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

âš ï¸ í•™ìŠµìžê°€ "ëª¨ë¥´ê² ì–´", "ìž˜ ëª¨ë¥´ê² ì–´ìš”" ê°™ì€ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•˜ë©´, ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ì‚¬ê³ ë¥¼ ì´‰ì§„í•˜ì„¸ìš”."""
                message_label = "í•™ìŠµìžì˜ í˜„ìž¬ ì‘ë‹µ"

            # Construct prompt
            prompt = f"""{system_prompt_to_use}

ì´ì „ ëŒ€í™”:
{context}

{message_label}: "{user_message}"

{instruction}"""

            # Generate response with timeout and error handling
            logger.info(f"Sending request to Gemini API for message: {user_message[:50]}...")
            response = self.model.generate_content(prompt)

            if not response or not response.text:
                logger.error("Gemini API returned empty response")
                return self._create_fallback_response(user_message)

            result_text = response.text
            logger.debug(f"Raw Gemini response (first 200 chars): {result_text[:200]}")

            # Parse JSON response
            # Remove markdown code blocks if present
            if "```json" in result_text:
                result_text = result_text.split("```json")[1].split("```")[0].strip()
            elif "```" in result_text:
                result_text = result_text.split("```")[1].split("```")[0].strip()

            result = json.loads(result_text)

            # Validate required fields based on mode
            if is_question:
                required_fields = ["current_stage", "detected_metacog_needs", "response_depth",
                                 "answer_message", "should_transition", "reasoning"]
                # Ensure we have answer_message and convert to scaffolding_question for consistency
                if "answer_message" in result:
                    # Combine answer with follow-up question if present
                    answer_text = result["answer_message"]
                    if "follow_up_question" in result and result["follow_up_question"]:
                        answer_text += " " + result["follow_up_question"]
                    result["scaffolding_question"] = answer_text
            else:
                required_fields = ["current_stage", "detected_metacog_needs", "response_depth",
                                 "scaffolding_question", "should_transition", "reasoning"]

            missing_fields = [field for field in required_fields if field not in result]
            if missing_fields:
                logger.error(f"Missing required fields in Gemini response: {missing_fields}")
                logger.error(f"Received result: {result}")
                return self._create_fallback_response(user_message)

            # Post-process: Ensure detected_metacog_needs is always a list
            if "detected_metacog_needs" in result:
                if isinstance(result["detected_metacog_needs"], str):
                    # Convert string to list
                    result["detected_metacog_needs"] = [result["detected_metacog_needs"]]
                    logger.warning(f"Converted detected_metacog_needs from string to list: {result['detected_metacog_needs']}")

                # Validate it's not empty
                if not result["detected_metacog_needs"]:
                    logger.warning("Empty detected_metacog_needs, setting default to 'ì ê²€'")
                    result["detected_metacog_needs"] = ["ì ê²€"]

            logger.info(f"Successfully generated scaffolding for stage: {result.get('current_stage')}, depth: {result.get('response_depth')}")
            return result

        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse Gemini response as JSON: {e}", exc_info=True)
            logger.error(f"Raw response: {response.text if 'response' in locals() else 'N/A'}")
            # Fallback response
            return self._create_fallback_response(user_message)

        except AttributeError as e:
            logger.error(f"Gemini API response format error: {e}", exc_info=True)
            return self._create_fallback_response(user_message)

        except Exception as e:
            logger.error(f"Unexpected error generating scaffolding: {e}", exc_info=True)
            logger.error(f"User message: {user_message}")
            logger.error(f"Conversation history length: {len(conversation_history)}")
            return self._create_fallback_response(user_message)

    def _build_context(
        self,
        conversation_history: List[Dict[str, str]],
        current_stage: Optional[str]
    ) -> str:
        """Build conversation context string"""
        if not conversation_history:
            return "ì—†ìŒ (ì²« ëŒ€í™”)"

        context_parts = []
        for msg in conversation_history[-MAX_CONTEXT_MESSAGES:]:
            role = "í•™ìŠµìž" if msg["role"] == "user" else "ì—ì´ì „íŠ¸"
            context_parts.append(f"{role}: {msg['content']}")

        if current_stage:
            context_parts.append(f"\ní˜„ìž¬ ë‹¨ê³„: {current_stage}")

        return "\n".join(context_parts)

    def _create_fallback_response(self, user_message: str) -> Dict:
        """Create fallback response when Gemini fails

        Provides a safe, general scaffolding question that can work in any situation.
        """
        logger.warning(f"Using fallback response for message: {user_message[:100]}")

        # Different fallbacks based on message length
        if len(user_message.strip()) < 10:
            question = "ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”? ì–´ë–¤ ìƒí™©ì¸ì§€ ë§ì”€í•´ì£¼ì„¸ìš”."
        else:
            question = "ë§ì”€í•´ì£¼ì‹  ë‚´ìš©ì— ëŒ€í•´ ì¡°ê¸ˆ ë” ìžì„¸ížˆ ì´ì•¼ê¸°í•´ë³¼ê¹Œìš”? ì–´ë–¤ ë¶€ë¶„ì´ ê°€ìž¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ì‹œë‚˜ìš”?"

        return {
            "current_stage": "ë„ì „_ì´í•´",
            "detected_metacog_needs": ["ì ê²€"],
            "response_depth": "medium",
            "scaffolding_question": question,
            "should_transition": False,
            "reasoning": "ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•œ ì•ˆì „í•œ ê¸°ë³¸ ì‘ë‹µ ì œê³µ"
        }


# Global service instance
gemini_service = GeminiService()
